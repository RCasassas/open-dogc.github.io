<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="open-dogc.GitHub.io : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/styles.css">

    <title>Open DOGC</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/open-dogc"></a>

          <h1 id="project_title">Open DOGC</h1>
          
          <ul class="menu-nav">
            <li><a href="#home">Home</a></li>
            <li><a href="#data">Data</a></li>
            <li><a href="#classification">Classification</a></li>
            <li><a href="#vector_machine">Vector Machine</a></li>
            <li><a href="#viz">Viz</a></li>
          </ul>
          a
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <div id="main_content" class="inner">
        <span class="anchor" id="home"></span>
        <div class="section">
<p>What do we know about our politicians? Do we have enough information about what decisions they make? Are journals really telling us everything that should be important?</p>

<p>What if we could analyze the information from official documents in a more efficient way without even need to read what’s inside?</p>

<p>In this project we will use python code to go through the official documents published in the website <a href="http://dogc.gencat.cat">http://dogc.gencat.cat</a> and return information about what these documents are talking about and about which persons are cited in the official documents.</p>

<p>The dogc website is where all official documents from the Catalan government of Generalitat de Catalunya are published. These documents refer to announcements, adjudications, decrees or agreements among others. We focused on those documents labeled as Agreements, as they are related to important decisions taken inside the government.</p>

<p>The project is one of the capstone project of the course Data Science and Big Data from the Universitat de Barcelona, and our group is composed by:</p>

<ul>
  <li><a href="https://es.linkedin.com/in/alexandra-ab%25C3%25B3s-ortega-3167a994" target="_blank">Alexandra Abós</a></li>
  <li><a href="https://es.linkedin.com/in/elisenda-renteria-255093a" target="_blank">Elisenda Rentería</a></li>
  <li><a href="https://es.linkedin.com/in/maria-francisca-peric%C3%A0s-19338ba6" target="_blank">Maria Francisca Pericàs</a></li>
  <li><a href="https://es.linkedin.com/in/ramon-casassas-garcia-339a09a9" target="_blank">Ramón Casassas</a></li>
</ul>

</div>

<span class="anchor" id="data"></span>
<div class="section">
<h3>Data</h3>

<p>To obtain official documents that are available in the website we need to scrap them. To do that we used the advanced search, that allowed us to be more specific about the kind of documents we wanted to analyze.</p> 

<p>Documents are written in Catalan, although they can also be downloaded in Spanish. However, the Spanish download returns part of the text in Catalan. Catalan is a difficult language to apply cleaning packages of text in python, and a text with two languages also implies other challenges. To solve that, we translated all documents to English in order to apply the cleaning techniques available in python. Translation was performed using textblob.</p>

<p>Once all documents are downloaded, the text inside the document needs to be cleaned in order to be analyzed. Data cleaning was performed in several steps:</p>

<ol>
  <li>Tokenization</li>
  <li>Removal of punctuation</li>
  <li>Removal of unnecessary words</li>
  <li>Stemming of words</li>
</ol>


<p>The python packages to apply this points were nltk, string and textblob.</p>
</div>

<span class="anchor" id="classification"></span>
<div class="section">
<h3>Classification</h3>

<p>IOnce the text inside each document is cleaned, these were classified based in the presence of key words in each text. To do that we had to create first vectors for each text to measure the frequency of the same (or similar) words inside each one. And after that we applied a topic model using a python program called lda to classify all documents by topics characterized by a group of words.</p>
</div>


<span class="anchor" id="vector_machine"></span>
<div class="section">
<h3>Vector Machine</h3>

</div>

<span class="anchor" id="viz"></span>
<div class="section">
<h3>Viz</h3>
<!-- Introdueixo un iframe per ficar el gràfic aqui dincs -->
<iframe src="plot1.html" style="width: 960px; height: 960px;"></iframe>
      </div>
      </div>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  

</body></html>
